# -*- coding: utf-8 -*-
"""Facial Key Points Detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BPR180BNS4IY1-IvknOBXbSqHdr80mmx

#Understanding the Dataset
"""

import pandas as pd
import matplotlib.pyplot as plt
import os
import cv2
import numpy as np



"""**Dataset is taken from [kaggle](https://www.kaggle.com/c/facial-keypoints-detection/overview)**"""

def plot(image,key_pts):
      plt.imshow(image)
      plt.scatter(key_pts[:,0],key_pts[:,1],s=5,marker='.',c='m')

def visualize(csv_path,root_dir,index):
      train_csv = pd.read_csv(csv_path)
      n = index
      image_name = train_csv.iloc[n,0]
      key_pts = train_csv.iloc[n,1:].values
      key_pts = key_pts.astype('float').reshape(-1, 2)
      image = cv2.imread(os.path.join(root_dir,image_name))
      plot(image,key_pts)

visualize('P1_Facial_Keypoints/data/training_frames_keypoints.csv'
            ,'P1_Facial_Keypoints/data/training',100)

import torch
from torch.utils.data import Dataset,DataLoader

class FacialKeyPointsDataset(torch.utils.data.Dataset):
  def __init__(self,csv_path,root_dir,transform=None):
    self.csv_file = pd.read_csv(csv_path)
    self.root_dir = root_dir
    self.transform = transform

  def __len__(self):
    return len(self.csv_file)

  def __getitem__(self,idx):
    image = cv2.imread(os.path.join(self.root_dir,
                                    self.csv_file.iloc[idx,0]))
    
    #If Image has alpha channel,neglect it
    if (image.shape[2]==4):
      image = image[:,:,0:3]

    key_pts = self.csv_file.iloc[idx,1:].values
    key_pts = key_pts.astype('float').reshape(-1, 2)
    sample = {'image':image,'keypoints':key_pts}

    if self.transform != None:
      sample = self.transform(sample)

    return sample

face_dataset = FacialKeyPointsDataset('P1_Facial_Keypoints/data/training_frames_keypoints.csv',
                                      'P1_Facial_Keypoints/data/training')
print(len(face_dataset))

#Display an Image
plot(face_dataset[200]['image'],face_dataset[200]['keypoints'])

"""# Transformations"""

#With some help from the open source community
import torch
from torchvision import transforms, utils
# tranforms

class Normalize(object):
    """Convert a color image to grayscale and normalize the color range to [0,1]."""        

    def __call__(self, sample):
        image, key_pts = sample['image'], sample['keypoints']
        
        image_copy = np.copy(image)
        key_pts_copy = np.copy(key_pts)

        # convert image to grayscale
        image_copy = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
        
        # scale color range from [0, 255] to [0, 1]
        image_copy=  image_copy/255.0
        
        # scale keypoints to be centered around 0 with a range of [-1, 1]
        # mean = 100, sqrt = 50, so, pts should be (pts - 100)/50
        key_pts_copy = (key_pts_copy - 100)/50.0


        return {'image': image_copy, 'keypoints': key_pts_copy}


class Rescale(object):
    """Rescale the image in a sample to a given size.

    Args:
        output_size (tuple or int): Desired output size. If tuple, output is
            matched to output_size. If int, smaller of image edges is matched
            to output_size keeping aspect ratio the same.
    """

    def __init__(self, output_size):
        assert isinstance(output_size, (int, tuple))
        self.output_size = output_size

    def __call__(self, sample):
        image, key_pts = sample['image'], sample['keypoints']

        h, w = image.shape[:2]
        if isinstance(self.output_size, int):
            if h > w:
                new_h, new_w = self.output_size * h / w, self.output_size
            else:
                new_h, new_w = self.output_size, self.output_size * w / h
        else:
            new_h, new_w = self.output_size

        new_h, new_w = int(new_h), int(new_w)

        img = cv2.resize(image, (new_w, new_h))
        
        # scale the pts, too
        key_pts = key_pts * [new_w / w, new_h / h]

        return {'image': img, 'keypoints': key_pts}


class RandomCrop(object):
    """Crop randomly the image in a sample.

    Args:
        output_size (tuple or int): Desired output size. If int, square crop
            is made.
    """

    def __init__(self, output_size):
        assert isinstance(output_size, (int, tuple))
        if isinstance(output_size, int):
            self.output_size = (output_size, output_size)
        else:
            assert len(output_size) == 2
            self.output_size = output_size

    def __call__(self, sample):
        image, key_pts = sample['image'], sample['keypoints']

        h, w = image.shape[:2]
        new_h, new_w = self.output_size

        top = np.random.randint(0, h - new_h)
        left = np.random.randint(0, w - new_w)

        image = image[top: top + new_h,
                      left: left + new_w]

        key_pts = key_pts - [left, top]

        return {'image': image, 'keypoints': key_pts}


class ToTensor(object):
    """Convert ndarrays in sample to Tensors."""

    def __call__(self, sample):
        image, key_pts = sample['image'], sample['keypoints']
         
        # if image has no grayscale color channel, add one
        if(len(image.shape) == 2):
            # add that third color dim
            image = image.reshape(image.shape[0], image.shape[1], 1)
            
        # swap color axis because
        # numpy image: H x W x C
        # torch image: C X H X W
        image = image.transpose((2, 0, 1))
        
        return {'image': torch.from_numpy(image),
                'keypoints': torch.from_numpy(key_pts)}

"""#Dataset"""

# test out some of these transforms
rescale = Rescale(100)
crop = RandomCrop(50)
gray = Normalize()
composed = transforms.Compose([
                               Rescale(100),
                               RandomCrop(96),
                               Normalize()
                               ])

# apply the transforms to a sample image
test_num = 100
sample = face_dataset[test_num]

fig = plt.figure()
for i, tx in enumerate([rescale, crop,gray, composed]):
    transformed_sample = tx(sample)

    ax = plt.subplot(1, 4, i + 1)
    plt.tight_layout()
    ax.set_title(type(tx).__name__)
    plot(transformed_sample['image'], (transformed_sample['keypoints']*50.0+100))

plt.show()

"""# Model Architecture"""

import torch
from torch import nn
import torch.nn.functional as F

class Network(torch.nn.Module):
  def __init__(self):
    super().__init__()
    self.conv1 = nn.Conv2d(1,32,3,padding=1)
    self.batchnorm1 = nn.BatchNorm2d(32)
    self.conv2 = nn.Conv2d(32,32,3,padding=1)
    self.batchnorm2 = nn.BatchNorm2d(32)
    self.maxpool1 = nn.MaxPool2d(2)

    self.conv3 = nn.Conv2d(32,64,3,padding=1)
    self.batchnorm3 = nn.BatchNorm2d(64)
    self.conv4 = nn.Conv2d(64,64,3,padding=1)
    self.batchnorm4 = nn.BatchNorm2d(64)
    self.maxpool2 = nn.MaxPool2d(2)

    self.conv5 = nn.Conv2d(64,128,3,padding=1)
    self.batchnorm5 = nn.BatchNorm2d(128)
    self.conv6 = nn.Conv2d(128,128,3,padding=1)
    self.batchnorm6 = nn.BatchNorm2d(128)
    self.maxpool3 = nn.MaxPool2d(2)

    self.conv7 = nn.Conv2d(128,256,3,padding=1)
    self.batchnorm7 = nn.BatchNorm2d(256)
    self.maxpool4 = nn.MaxPool2d(2)
    self.conv8 = nn.Conv2d(256,512,3,padding=1)
    self.batchnorm8 = nn.BatchNorm2d(512)
    self.maxpool5 = nn.MaxPool2d(2)
    self.drop1 = nn.Dropout2d(0.3)

    # self.conv9 = nn.Conv2d(256,512,3,padding=1)
    # self.batchnorm9 = nn.BatchNorm2d(512)
    # self.conv10 = nn.Conv2d(512,512,3,padding=1)
    # self.batchnorm10 = nn.BatchNorm2d(512)
    # self.maxpool5 = nn.MaxPool2d(2)
    # self.drop1 = nn.Dropout2d(0.4)

    self.dense1 = nn.Linear(4608,1000)
    self.drop2 = nn.Dropout2d(0.2)

    self.dense2 = nn.Linear(1000,500)
    self.drop3 = nn.Dropout2d(0.2)


    self.dense3 = nn.Linear(500,136)


  def forward(self,x):
    x = self.maxpool1(self.batchnorm2(F.relu((self.conv2(self.batchnorm1(F.relu(self.conv1(x))))))))
    x = self.maxpool2(self.batchnorm4(F.relu((self.conv4(self.batchnorm3(F.relu(self.conv3(x))))))))
    x = self.maxpool3(self.batchnorm6(F.relu((self.conv6(self.batchnorm5(F.relu(self.conv5(x))))))))
    x = self.drop1(self.maxpool5(self.batchnorm8(F.relu((self.conv8(
        self.maxpool4(self.batchnorm7(F.relu(self.conv7(x))))))))))
    # x = self.maxpool5(self.batchnorm10(F.relu((self.conv10(self.batchnorm9(F.relu(self.conv9(x))))))))

    x = x.view(x.size(0),-1)

    x = self.drop2(F.relu(self.dense1(x)))
    x = self.drop3(F.relu(self.dense2(x)))  
    x = self.dense3(x) 

    return x

net = Network()
net

for layer in net.modules():
  if(type(layer) != Network):
    if(type(layer)!=nn.MaxPool2d and type(layer)!=nn.BatchNorm2d
       and type(layer)!=nn.Dropout2d):
          nn.init.xavier_uniform_(layer.weight)
          print("Done")

"""# Dataset Loader"""

data_transform = transforms.Compose([
                                     Rescale(100),
                                     RandomCrop(96),
                                     Normalize(),
                                     ToTensor()
                                        ])

transformed_dataset = FacialKeyPointsDataset('P1_Facial_Keypoints/data/training_frames_keypoints.csv',
                                      'P1_Facial_Keypoints/data/training',data_transform)
train_loader = torch.utils.data.DataLoader(transformed_dataset,64,shuffle=True,num_workers=4)


transformed_dataset_t = FacialKeyPointsDataset('P1_Facial_Keypoints/data/test_frames_keypoints.csv',
                                      'P1_Facial_Keypoints/data/test',data_transform)
test_loader = torch.utils.data.DataLoader(transformed_dataset_t,64,num_workers=4)

temp = next(iter(train_loader))
images,key_pts = temp['image'],temp['keypoints']
image=images[1].permute(1,2,0).squeeze()
plot(image,key_pts[1]*50+100)

"""#Training the Model"""

from torch import optim
criterian = nn.MSELoss()
optimizer = optim.RMSprop(net.parameters(),lr=0.001)

def train(epochs,net):
  net.train()
  net.to('cuda')
  for i in range(epochs):
      running_loss = 0.0
      for ii,samples in enumerate(iter(train_loader)):
        optimizer.zero_grad()
        images = samples['image']
        labels = samples['keypoints']

        #Flatten Key Points
        labels = labels.view(labels.size(0),-1)
        
        labels = labels.type(torch.FloatTensor)
        images = images.type(torch.FloatTensor)
        images,labels = images.to('cuda'),labels.to('cuda')

        output_labels = net.forward(images)
        loss = criterian(output_labels,labels)

        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        
      print("Epoch {}.........Loss {:.5f}".format(i,running_loss))
      running_loss = 0.0
  print("Fininshed Training")

epochs = 100
train(epochs=epochs,net=net)

"""#Saving and Loading the Model"""

#In case
from google.colab import drive
drive.mount('/content/gdrive')

torch.save(net.state_dict(),"keypoints_model.pth")

saved = torch.load("keypoints_model.pth",map_location='cpu')
net.load_state_dict(saved)

"""#Testing"""

index=22
net.eval()
#net.to('cuda')
temp = next(iter(test_loader))
images,key_pts = temp['image'],temp['keypoints']
out = net.forward(images[index].unsqueeze(0).type(torch.FloatTensor))
out = out.to('cpu').detach().numpy()
out = out.astype('float').reshape(-1,2)
image=images[index].permute(1,2,0).squeeze()

#Orginal
plot(image,key_pts[index]*50.0+100)

#Predicted
plot(image,out*50.0+100)

"""#Visualizing Features"""

weights1 = net.conv2.weight.data
w = weights1.numpy()
filter_idx = 1
print(w.shape)
print(w[filter_idx][0].shape)

plt.imshow(w[filter_idx][0],cmap='gray')

figure = plt.figure(figsize=(96,96))
for i in range(4):
      ax = figure.add_subplot(3,4,i+1,xticks=[],yticks=[])
      img = cv2.filter2D(image.numpy(),-1,w[filter_idx+i][0])
      ax.imshow(img)

"""#Complete Pipeline"""

#Test on an Image
image_path = "test3.jpg"
image = cv2.imread(image_path)
image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)
plt.imshow(image)

#Detecting Faces
image_copy = np.copy(cv2.cvtColor(image,cv2.COLOR_RGB2GRAY))
image_copy_2 = np.copy(image)
face_cascade = cv2.CascadeClassifier("haarcascade_frontalface_default.xml")

faces = face_cascade.detectMultiScale(image_copy,1.3,5)
offset = int((image_copy_2.shape[0]/100)*10)
for (x,y,w,h) in faces:
  cv2.rectangle(image_copy_2,(x-offset,y-offset),(x+w+offset,y+h+offset),(255,0,0),10)

plt.imshow(image_copy_2)

#cropping
face = image_copy_2[y-offset:y+h+offset,x-offset:x+h+offset]
plt.imshow(face)

#Doing Some Preprocessing
face_copy = np.copy(face)
face_copy_2 = cv2.cvtColor(face_copy,cv2.COLOR_RGB2GRAY)
face_copy_2 = face_copy_2 / 255.0
face_copy_2 = cv2.resize(face_copy_2,(96,96))

face_copy_2 = torch.from_numpy(face_copy_2)
face_copy_2.unsqueeze_(0)

#Predicting key-points
pre_key_pts = net.forward(face_copy_2.unsqueeze(0).type(torch.FloatTensor))

#Doing some preprocessing again and then displaying the results
out = pre_key_pts.to('cpu').detach().numpy()
out = out.astype('float').reshape(-1,2)
key_pts = out*50.0+100
#image=face_copy_2.permute(1,2,0).squeeze()
plt.imshow(cv2.resize(face,(96,96)))
plt.scatter(key_pts[:,0],key_pts[:,1],s=10,marker='.',c='m')